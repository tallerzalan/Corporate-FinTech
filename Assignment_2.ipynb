{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporate FinTech Assignment 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marton Nemeth - manem18@student.sdu.dk\n",
    "\n",
    "Michal Minarcik - mimin18@student.sdu.dk\n",
    "\n",
    "Nicolas Hamann Hiemstra von Arenstorff - niare21@student.sdu.dk\n",
    "\n",
    "Toheed Adeola Musa - amusa18@student.sdu.dk\n",
    "\n",
    "Zalan Taller - zatal21@student.sdu.dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textual analysis or natural language processing (NLP) is a type of qualitative analysis increasingly applied more in finance research. In the field of behavioral finance, the researchers have been studying how sentiment impacts individual investors, institutions, and markets. Two types of sentiment have been identified: the investor sentiment and text-based or textual sentiment. Our focus falls solely on the latter, which measures the degree of positivity or negativity in texts. An interesting use case of the qualitative information obtained from textual analysis is the inclusion into equity asset pricing models, as it provides another perspective and potentially complementary information to quantitative information measures in the price formation process (Kearney and Liu, 2014).\n",
    "\n",
    "The qualitative information used by textual sentiment researchers in finance, comes predominantly from media articles, public corporate disclosures, and social media platforms.\n",
    "To measure the tone of the documents, the researchers commonly use either the dictionary-based approach or machine learning.\n",
    "\n",
    "The dictionary-based approach uses a mapping algorithm in which a computer program reads text and classifies words, phrases, or sentences into groups based on pre-defined dictionary categories (Li (2010)). One of the difficulties of this approach is that English words often have many meanings and a word classifier developed for one discipline might not produce good results for another. Loughran and McDonald (2011) show that the commonly used source for word classifications, the Harvard-IV-4 TagNeg (H4N) list, misclassifies words when assessing the tone in financial applications. The other common issue is how each word in the word list should be weighted.\n",
    "\n",
    "The machine learning approach relies on statistical techniques to infer the content of documents and to classify them based on statistical inference (Li, 2010). This method works as follows. A part of the text to be analyzed is chosen as the “training set”. Each word from this set is manually classified as ‘positive’, ‘negative’, or some other dimension of sentiment. A sentiment analysis algorithm, such as Naïve Bayesian algorithm, is then trained on this training set. The algorithm learns the sentiment classification rules from the pre-classified data set and applies these rules out-of-sample for the whole text to derive textual sentiment scores (Kearney and Liu, 2014).\n",
    "\n",
    "The Naive Bayesian has numerous advantages, Firstly, it can be seen as one of the most aged, well-established procedures to examine tests. secondly, With the use of machine algorithms, large corpuses of the data can be easily included in the test analysis, Thirdly, after the rules of gauging the test is specified, no supplementary researcher subjectivity impacts the measuring of tone in the business communication document while the weakness of Naive Bayesian is the complexity/numerous ways of establishing the gauging rules to measure the context of the document that may cause the difficulty of other users to replicate the results.\n",
    "\n",
    "The early use of the Naive Bayes approach dates back to 2004 when Antweiler and Frank examine 1.5 million stock messages posted on Yahoo Finance. They find that the number of posted messages is subsequently linked with the stock return volatility ( Antweiler and Frank [2004]).\n",
    "Another similar approach is Das and Chen [2007] which use NLP to measure sentiment in message postings for 24 high tech stocks. This research finds that stock message board postings are related to stock market levels, trading volume and volatility.\n",
    "\n",
    "A further approach of the Naïve Bayes method was by Li [2010b] that examine the content of the forward-looking statement in the MD&A section of the 10-K. The research used trained naive Bayes learning algorithms that were 30,000 randomly selected sentences coded manually to finds that the average tone of the FLS is positively linked with subsequent earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every non-missing rating review in the ICO dataset “icoData_19092018.json” and all ICOs that have ended (the ICO end date < 19-September-2018) we delete all ICO observations that have no ICO end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'icoData_19092018.json'\n",
    "\n",
    "with open(filename) as json_data:\n",
    "    icoData = json.load(json_data)\n",
    "\n",
    "# Deleting the rows without any information.\n",
    "icoData = [i for i in icoData if not len(i) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_dates = [d['dates'] for d in icoData]\n",
    "d_ratings = [d['ratings'][0] for d in icoData]\n",
    "\n",
    "df_d_dates = pd.DataFrame.from_dict(d_dates)\n",
    "df_d_ratings = pd.DataFrame.from_dict(d_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_d_dates, df_d_ratings], axis = 1)\n",
    "\n",
    "df_icoData = pd.DataFrame(icoData)\n",
    "df['overall_rating'] = df_icoData['rating']\n",
    "\n",
    "df['icoEnd'] = pd.to_datetime(df['icoEnd'], errors = 'coerce')\n",
    "df = df[(df['icoEnd'] < '2018-09-19')]\n",
    "nan_value = float('NaN')\n",
    "df.replace('', nan_value, inplace = True)\n",
    "df.dropna(subset = ['review'], inplace = True)\n",
    "df.dropna(subset = ['icoEnd'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reviews = df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reviews_str = str(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a.)\n",
    "We calculate the polarity scores using the pre-trained TextBlob classifier. We map polarity scores [-1, -0.25] to class “negative”, [-0.25, 0,25] to class “neutral”, and [0.25, 1] to class “positive” We report summary statistics for the resulting classes (mean, 25%-quartile, median, 50%-quartile, minimum, and maximum, and report the total number of observations), then we interpret the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sentiment(polarity=0.28750000000000003, subjectivity=0.6433333333333333)"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_sentiment = TextBlob(reviews_str).sentiment\n",
    "overall_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to get the polarity scores.\n",
    "\n",
    "def getPolarity(reviews):\n",
    "    return TextBlob(reviews).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new column called 'Polarity'.\n",
    "\n",
    "df['Polarity'] = df['review'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to label each review basis the polarity score.\n",
    "\n",
    "def getAnalysis(score):\n",
    "    if score <= -0.25:\n",
    "        return 'Negative'\n",
    "    elif score <= 0.25:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new column called 'Sentiment'.\n",
    "\n",
    "df['Sentiment'] = df['Polarity'].apply(getAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_grouped = df.groupby('Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Custom function to report summary statistics for the resulting classes.\n",
    "\n",
    "def describe(df):\n",
    "    return pd.concat([df.describe(), df.median().rename('median')], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           count      mean       std       min       25%       50%       75%  \\\nSentiment                                                                      \nNegative    12.0 -0.404470  0.105419 -0.500000 -0.500000 -0.436198 -0.295312   \nNeutral    284.0  0.081064  0.112130 -0.243750  0.000000  0.100000  0.166167   \nPositive   234.0  0.501137  0.191846  0.250213  0.360625  0.435000  0.616667   \n\n            max    median  \nSentiment                  \nNegative  -0.25 -0.436198  \nNeutral    0.25  0.100000  \nPositive   1.00  0.435000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n      <th>median</th>\n    </tr>\n    <tr>\n      <th>Sentiment</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Negative</th>\n      <td>12.0</td>\n      <td>-0.404470</td>\n      <td>0.105419</td>\n      <td>-0.500000</td>\n      <td>-0.500000</td>\n      <td>-0.436198</td>\n      <td>-0.295312</td>\n      <td>-0.25</td>\n      <td>-0.436198</td>\n    </tr>\n    <tr>\n      <th>Neutral</th>\n      <td>284.0</td>\n      <td>0.081064</td>\n      <td>0.112130</td>\n      <td>-0.243750</td>\n      <td>0.000000</td>\n      <td>0.100000</td>\n      <td>0.166167</td>\n      <td>0.25</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>Positive</th>\n      <td>234.0</td>\n      <td>0.501137</td>\n      <td>0.191846</td>\n      <td>0.250213</td>\n      <td>0.360625</td>\n      <td>0.435000</td>\n      <td>0.616667</td>\n      <td>1.00</td>\n      <td>0.435000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_statistics = describe(sentiment_grouped['Polarity'])\n",
    "summary_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained TextBlob classifier mapped the polarity scores of 530 reviews. We classified each review as negative, neutral or positive basis the polarity score. For each of the resulting classes we have the above descriptive statistics.\n",
    "\n",
    "The negative class ranges from -0.5 to -0.28 where the mean and the median are similar, although only 10 reviews were classified negative.\n",
    "The neutral class ranges from -0.25 to 0.249 where the mean and the median are also similar. The majority of the reviews were classified neutral.\n",
    "The positive ranges from 0.25 to 1 where the mean and the median are also similar. 240 reviews were classified positive.\n",
    "\n",
    "Assuming that the pre-trained TextBlob classifier is a trustworthy, we could argue that people do not write negative reviews in general, as only 10 reviews were classified negative, the majority of the reviews were classified neutral, and the number of positive reviews is 19x the number of negative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b.)\n",
    "We randomly draw 300 non-missing rating reviews and labelws them into three classes: “positive”, “neutral”, and “negative”. We used the first two thirds of the observations as training and the remaining third as test dataset. We used our training dataset to train a Naive Bayes classifier using TextBlob. We then used this classifier to classify the rating reviews in the test dataset and calculate the accuracy (precision, recall, and F1) metric for the test dataset. We interpreted the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reduced = df.loc[:, ['Sentiment', 'review']].copy()\n",
    "reduced.rename(columns = {'Sentiment' : 'target'}, inplace = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the train and test datasets from altogether 300 reviews. For the TextBlob classifier we need a list of doubles (string, target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reviews_zipped = [(s, t) for s, t in zip(reduced.review, reduced.target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly sampling 200 reviews for the train dataset.\n",
    "random.seed(5)\n",
    "\n",
    "train = random.sample(reviews_zipped, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reviews_without_train = [review for review in reviews_zipped if review not in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly sampling 100 reviews for the test dataset.\n",
    "random.seed(5)\n",
    "\n",
    "test = random.sample(reviews_without_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<NaiveBayesClassifier trained on 200 instances>"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Naive Bayes Classifier on 200 reviews.\n",
    "\n",
    "classifier = NaiveBayesClassifier(train)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.53"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the accuracy metric for the test dataset.\n",
    "\n",
    "accuracy_test = classifier.accuracy(test)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier is correct 53 of the time.\n"
     ]
    }
   ],
   "source": [
    "predicted_classifications = []\n",
    "\n",
    "def make_predictions():\n",
    "    for i in range(len(test)):\n",
    "        classification = classifier.classify(test[i][0])\n",
    "        predicted_classifications.append(classification)\n",
    "\n",
    "make_predictions()\n",
    "\n",
    "def accuracy():\n",
    "    countCorrect = 0\n",
    "    for i in range(len(test)):\n",
    "        if predicted_classifications[i] == test[i][1]:\n",
    "            countCorrect += 1\n",
    "    accuracy = int((countCorrect / len(predicted_classifications)) * 100)\n",
    "    print('Classifier is correct', accuracy, 'of the time.')\n",
    "\n",
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier is incorrect 47 of the time.\n"
     ]
    }
   ],
   "source": [
    "def return_incorrect():\n",
    "    incorrectIndexes = []\n",
    "    for i in range(len(test)):\n",
    "        if predicted_classifications[i] != test[i][1]:\n",
    "            incorrectIndexes.append(i)\n",
    "    print('Classifier is incorrect', len(incorrectIndexes), 'of the time.' )\n",
    "\n",
    "return_incorrect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = pd.Series(predicted_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Predicted  Negative  Neutral  Positive  All\nActual                                     \nNegative          0        0         2    2\nNeutral           1       14        44   59\nPositive          0        0        39   39\nAll               1       14        85  100",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Predicted</th>\n      <th>Negative</th>\n      <th>Neutral</th>\n      <th>Positive</th>\n      <th>All</th>\n    </tr>\n    <tr>\n      <th>Actual</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Negative</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>Neutral</th>\n      <td>1</td>\n      <td>14</td>\n      <td>44</td>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>Positive</th>\n      <td>0</td>\n      <td>0</td>\n      <td>39</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>All</th>\n      <td>1</td>\n      <td>14</td>\n      <td>85</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = pd.Series([x[1] for x in test])\n",
    "crosstab = pd.crosstab(actual, predicted, rownames = ['Actual'], colnames = ['Predicted'], margins = True)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "precision = crosstab.iloc[2].iloc[2] / crosstab.iloc[3].iloc[2]\n",
    "recall = crosstab.iloc[2].iloc[3] / (crosstab.iloc[2].iloc[3] + crosstab.iloc[1].iloc[0])\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "data": {
      "text/plain": "   Precision  Recall     F1\n0   0.458824   0.975  0.624",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.458824</td>\n      <td>0.975</td>\n      <td>0.624</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = pd.DataFrame(data = [precision, recall, f1]).T\n",
    "evaluation.columns = ['Precision', 'Recall', 'F1']\n",
    "evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = TP / (TP + FP) = 39 / (39 + 44 + 2) = 39 / 85 = 0.459 = 45.9%\n",
    "\n",
    "Recall = TP / (TP + FN) = 39 / (39 + 1) = 39 / 40 = 0.975 = 97.5%\n",
    "\n",
    "F1 = 2 * ((Precision * Recall) / (Precision + Recall)) = 2 * ((0.459 * 0.975) / (0.459 + 0.975)) = 2 * (0.447525 / 1.434) = 2 * 0.312= 0.624 = 62.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c.)\n",
    "We used our classifier from b.) to classify all non-missing rating reviews (that are not used as training and test data) and reported the same summary statistics as in a.).\n",
    "We interpreted the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "remaining_reviews = [review for review in reviews_without_train if review not in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.5963302752293578"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the accuracy metric for the rest of the reviews.\n",
    "\n",
    "accuracy_remaining_reviews = classifier.accuracy(remaining_reviews)\n",
    "accuracy_remaining_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_remaining_reviews = pd.DataFrame(remaining_reviews, columns = ['review', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "remaining_reviews_str = str(df_remaining_reviews['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sentiment(polarity=0.45173076923076927, subjectivity=0.5815384615384616)"
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = TextBlob(remaining_reviews_str).sentiment\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_remaining_reviews['Polarity'] = df_remaining_reviews['review'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_remaining_reviews['Sentiment'] = df_remaining_reviews['Polarity'].apply(getAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_sentiment_grouped = df_remaining_reviews.groupby('Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           count      mean       std   min   25%       50%       75%      max  \\\nSentiment                                                                       \nNegative     5.0 -0.456250  0.097828 -0.50 -0.50 -0.500000 -0.500000 -0.28125   \nNeutral    112.0  0.090806  0.103465 -0.17  0.00  0.095402  0.173256  0.25000   \nPositive   101.0  0.494325  0.187470  0.26  0.35  0.436667  0.550000  1.00000   \n\n             median  \nSentiment            \nNegative  -0.500000  \nNeutral    0.095402  \nPositive   0.436667  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n      <th>median</th>\n    </tr>\n    <tr>\n      <th>Sentiment</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Negative</th>\n      <td>5.0</td>\n      <td>-0.456250</td>\n      <td>0.097828</td>\n      <td>-0.50</td>\n      <td>-0.50</td>\n      <td>-0.500000</td>\n      <td>-0.500000</td>\n      <td>-0.28125</td>\n      <td>-0.500000</td>\n    </tr>\n    <tr>\n      <th>Neutral</th>\n      <td>112.0</td>\n      <td>0.090806</td>\n      <td>0.103465</td>\n      <td>-0.17</td>\n      <td>0.00</td>\n      <td>0.095402</td>\n      <td>0.173256</td>\n      <td>0.25000</td>\n      <td>0.095402</td>\n    </tr>\n    <tr>\n      <th>Positive</th>\n      <td>101.0</td>\n      <td>0.494325</td>\n      <td>0.187470</td>\n      <td>0.26</td>\n      <td>0.35</td>\n      <td>0.436667</td>\n      <td>0.550000</td>\n      <td>1.00000</td>\n      <td>0.436667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_summary_statistics = describe(new_sentiment_grouped['Polarity'])\n",
    "new_summary_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained TextBlob classifier mapped the polarity score of the 227 reviews that are neither in the train nor in the test dateset. We classified each review as negative, neutral or positive basis the polarity score. For each of the resulting classes we have the above descriptive statistics.\n",
    "\n",
    "The negative class ranges from -0.5 to -0.3 where the mean and the median are similar, although only 6 reviews were classified negative.\n",
    "The neutral class ranges from -0.24 to 0.24 where the mean and the median are almost the same. The majority of the reviews were classified neutral.\n",
    "The positive ranges from 0.25 to 1 where the mean and the median are more or less similar. 101 reviews were classified positive.\n",
    "\n",
    "Once again assuming that the pre-trained TextBlob classifier is trustworthy, we could argue that people do not write negative reviews in general, as only 6 reviews were classified negative, the majority of the reviews were classified neutral, and the number of positive reviews is almost 17x the number of negative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.d.)\n",
    "We compared the pre-trained classification from a.) with our classification in b.) for the 300 observations in b.). We calculated the accuracy (precision, recall, and F1) metric for the pre-trained classifier. We interpreted the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier is correct 59 of the time.\n"
     ]
    }
   ],
   "source": [
    "predicted_classifications_remaining_reviews = []\n",
    "\n",
    "def make_predictions():\n",
    "    for i in range(len(remaining_reviews)):\n",
    "        classification = classifier.classify(remaining_reviews[i][0])\n",
    "        predicted_classifications_remaining_reviews.append(classification)\n",
    "\n",
    "make_predictions()\n",
    "\n",
    "def accuracy():\n",
    "    countCorrect = 0\n",
    "    for i in range(len(remaining_reviews)):\n",
    "        if predicted_classifications_remaining_reviews[i] == remaining_reviews[i][1]:\n",
    "            countCorrect += 1\n",
    "    accuracy = int((countCorrect / len(predicted_classifications_remaining_reviews)) * 100)\n",
    "    print('Classifier is correct', accuracy, 'of the time.')\n",
    "\n",
    "accuracy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "predicted_remaining_reviews = pd.Series(predicted_classifications_remaining_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "data": {
      "text/plain": "Predicted  Negative  Neutral  Positive  All\nActual                                     \nNegative          0        0         5    5\nNeutral           2       37        73  112\nPositive          0        8        93  101\nAll               2       45       171  218",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Predicted</th>\n      <th>Negative</th>\n      <th>Neutral</th>\n      <th>Positive</th>\n      <th>All</th>\n    </tr>\n    <tr>\n      <th>Actual</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Negative</th>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Neutral</th>\n      <td>2</td>\n      <td>37</td>\n      <td>73</td>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>Positive</th>\n      <td>0</td>\n      <td>8</td>\n      <td>93</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>All</th>\n      <td>2</td>\n      <td>45</td>\n      <td>171</td>\n      <td>218</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_remaining_reviews = pd.Series([x[1] for x in remaining_reviews])\n",
    "crosstab_remaining_reviews = pd.crosstab(actual_remaining_reviews, predicted_remaining_reviews, rownames = ['Actual'], colnames = ['Predicted'], margins = True)\n",
    "crosstab_remaining_reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "precision_remaining_reviews = crosstab_remaining_reviews.iloc[2].iloc[2] / crosstab_remaining_reviews.iloc[3].iloc[2]\n",
    "recall_remaining_reviews = crosstab_remaining_reviews.iloc[2].iloc[2] / (crosstab_remaining_reviews.iloc[2].iloc[2] + crosstab_remaining_reviews.iloc[1].iloc[0])\n",
    "f1_remaining_reviews = 2 * ((precision_remaining_reviews * recall_remaining_reviews) / (precision_remaining_reviews + recall_remaining_reviews))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "data": {
      "text/plain": "   Precision    Recall        F1\n0    0.54386  0.978947  0.699248",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.54386</td>\n      <td>0.978947</td>\n      <td>0.699248</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_remaining_reviews = pd.DataFrame(data = [precision_remaining_reviews, recall_remaining_reviews, f1_remaining_reviews]).T\n",
    "evaluation_remaining_reviews.columns = ['Precision', 'Recall', 'F1']\n",
    "evaluation_remaining_reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Precision = TP / (TP + FP) = 93 / (93 + 73 + 5) = 93 / 171 = 0.5438 = 54.38%\n",
    "\n",
    "Recall = TP / (TP + FN) = 93 / (93 + 2) = 93 / 95 = 0.9789 = 97.89%\n",
    "\n",
    "F1 = 2 * ((Precision * Recall) / (Precision + Recall)) = 2 * ((0.5438 * 0.9789) / (0.5438 + 0.9789)) = 2 * (0.5323 / 1.5227) = 2 * 0.34957= 0.699 = 69.9%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}